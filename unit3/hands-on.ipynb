{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "import gym"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 25\n",
    "n_jobs = 1\n",
    "n_startup_trials = 5\n",
    "n_evaluations = 8\n",
    "n_timesteps = 1000000\n",
    "eval_freq = int(n_timesteps / n_evaluations)\n",
    "n_eval_envs = 5\n",
    "n_eval_episodes = 10\n",
    "\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "\n",
    "default_hyperparams = {\n",
    "    \"policy\": \"MlpPolicy\",\n",
    "    \"env\": env_id,\n",
    "    \"n_steps\": 1024,\n",
    "    \"batch_size\": 256,\n",
    "    \"n_epochs\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ppo_params(trial: optuna.Trial) -> dict:\n",
    "    \"\"\"\n",
    "    return: The sampled hyperparameters for the given trial.\n",
    "    \"\"\"\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32, 64, 128])\n",
    "    gamma = 1 - trial.suggest_float(\"gamma\", 1e-5, 1e-1, log=True)\n",
    "    gae_lambda = 1 - trial.suggest_float(\"gae_lambda\", 1e-5, 1e-1, log=True)\n",
    "    ent_coef = trial.suggest_float(\"ent_coef\", 1e-4, 1e-1, log=True)\n",
    "\n",
    "    \n",
    "    return {\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"gamma\": gamma,\n",
    "        \"gae_lambda\": gae_lambda,\n",
    "        \"ent_coef\": ent_coef,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "class TrialEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Callback used for evaluating and reporting a trial.\n",
    "    \n",
    "    :param eval_env: Evaluation environement\n",
    "    :param trial: Optuna trial object\n",
    "    :param n_eval_episodes: Number of evaluation episodes\n",
    "    :param eval_freq:   Evaluate the agent every ``eval_freq`` call of the callback.\n",
    "    :param deterministic: Whether the evaluation should\n",
    "        use a stochastic or deterministic policy.\n",
    "    :param verbose:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eval_env: gym.Env,\n",
    "        trial: optuna.Trial,\n",
    "        n_eval_episodes: int = 5,\n",
    "        eval_freq: int = 10000,\n",
    "        deterministic: bool = True,\n",
    "        verbose: int = 0,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            eval_env=eval_env,\n",
    "            n_eval_episodes=n_eval_episodes,\n",
    "            eval_freq=eval_freq,\n",
    "            deterministic=deterministic,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        self.trial = trial\n",
    "        self.eval_idx = 0\n",
    "        self.is_pruned = False\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Evaluate policy (done in the parent class)\n",
    "            super()._on_step()\n",
    "            self.eval_idx += 1\n",
    "            # Send report to Optuna\n",
    "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
    "            # Prune trial if need\n",
    "            if self.trial.should_prune():\n",
    "                self.is_pruned = True\n",
    "                return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "def objective(trial: optuna.Trial) -> float:\n",
    "    \"\"\"\n",
    "    return: The mean reward of the evaluated agent.\n",
    "    \"\"\"\n",
    "\n",
    "    hyperparams = default_hyperparams.copy()\n",
    "    hyperparams.update(sample_ppo_params(trial))\n",
    "    \n",
    "    model = PPO(**hyperparams)\n",
    "    \n",
    "    eval_envs = make_vec_env(env_id, n_envs=n_eval_envs)\n",
    "    \n",
    "    eval_callback = TrialEvalCallback(\n",
    "        eval_env=eval_envs,\n",
    "        trial=trial,\n",
    "        n_eval_episodes=n_eval_episodes,\n",
    "        eval_freq=eval_freq,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    nan_encountered = False\n",
    "    try:\n",
    "        # Train the model\n",
    "        model.learn(n_timesteps, callback=eval_callback)\n",
    "    except AssertionError as e:\n",
    "        # Sometimes, random hyperparams can generate NaN\n",
    "        print(e)\n",
    "        nan_encountered = True\n",
    "    finally:\n",
    "        # Free memory\n",
    "        model.env.close()\n",
    "        eval_envs.close()\n",
    "\n",
    "    # Tell the optimizer that the trial failed\n",
    "    if nan_encountered:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    if eval_callback.is_pruned:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return eval_callback.last_mean_reward    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)\n",
    "\n",
    "sampler = TPESampler(n_startup_trials=n_startup_trials)\n",
    "pruner = MedianPruner(n_startup_trials=n_startup_trials, n_warmup_steps=n_evaluations // 3)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    "    direction=\"maximize\",\n",
    ")\n",
    "\n",
    "\n",
    "try:\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=n_trials,\n",
    "        n_jobs=n_jobs,\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "\n",
    "\n",
    "print(\"Number of finished trials: \", len(study.trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "print(\"  User attrs:\")\n",
    "for key, value in trial.user_attrs.items():\n",
    "    print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "# Write report\n",
    "study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n",
    "\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig2 = plot_param_importances(study)\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_num_threads(4)\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=16,\n",
    "    n_epochs=4,\n",
    "    learning_rate=0.0006877588267892911,\n",
    "    gamma=0.0103131621676421333,\n",
    "    gae_lambda=1.9666131053362384e-05,\n",
    "    ent_coef=0.00020273282332977947,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "model.learn(10_000_000)\n",
    "\n",
    "model_name = \"ppo_lunarlander-v2\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"ppo_lunarlander-v2.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akghxhs55/study/hugging-face/deep-rl-course/unit3/.env/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-20.97 +/- 156.19383567103202\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "eval_env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d832c146b2bac3d7cfdfc20dc302498f1bcdf4143c1ccec028802b73aba81a27"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
